{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "730b768a-6469-454c-b729-a278debc9bae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from io import BytesIO\n",
    "from utils.constants import *\n",
    "import openpyxl\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "# ========================\n",
    "# GENERAL\n",
    "# ========================\n",
    "\n",
    "# Return df bytes by saving in memory without saving in disk\n",
    "def get_df_bytes(df: pd.DataFrame) -> bytes:\n",
    "\n",
    "    # Save df to buffer to get df bytes\n",
    "    buffer = BytesIO()\n",
    "    # Convert DataFrame to parquet format in memory\n",
    "    df.to_parquet(buffer, index=False)\n",
    "    # Get bytes from buffer\n",
    "    df_bytes = buffer.getvalue()\n",
    "\n",
    "    # Return the bytes\n",
    "    return BytesIO(df_bytes)\n",
    "\n",
    "# Convert openpyxl workbook to BytesIO object\n",
    "def get_sheet_bytes(wb: openpyxl.workbook.workbook.Workbook) -> bytes:\n",
    "\n",
    "    # Save sheet to buffer to get sheet bytes\n",
    "    buffer = BytesIO()\n",
    "    # Save workbook to buffer\n",
    "    wb.save(buffer)\n",
    "    # Get bytes from buffer\n",
    "    sheet_bytes = buffer.getvalue()\n",
    "    # After using the workbook, it is necessary to close it\n",
    "    wb.close()\n",
    "\n",
    "    # Return BytesIO object with sheet bytes\n",
    "    return BytesIO(sheet_bytes)\n",
    "\n",
    "# ========================\n",
    "# TO LAYERS\n",
    "# ========================\n",
    "\n",
    "# Save sheet in ADLS as parquet\n",
    "# Excel bytes -> DataFrame -> Parquet bytes -> ADLS\n",
    "def save_sheet(wb, year: str, month: str):\n",
    "\n",
    "    # Create path\n",
    "    path = f'{DEV_CONTAINER}{ADLS_LAYER_SILVER}/{ADLS_CATEGORY_SALES}/{year}/{month}'\n",
    "\n",
    "    # Get sheet bytes\n",
    "    sheet_bytes = get_sheet_bytes(wb)\n",
    "\n",
    "    # Read sheet as df\n",
    "    sheet_df = ps.read_excel(sheet_bytes)\n",
    "\n",
    "    # Save df\n",
    "    sheet_df.to_parquet(path, index=False)\n",
    "\n",
    "# Save df in ADLS as parquet\n",
    "# DataFrame -> Parquet bytes -> ADLS\n",
    "# def save_df(df: pd.DataFrame, layer: str, category: str, year: str, month: str):\n",
    "\n",
    "#     path = f'{layer}/{category}/{year}/{month}/{category}_{year}_{month}{TABULAR_STD_EXTENSION}'\n",
    "\n",
    "#     # Get df parquet bytes\n",
    "#     parquet_bytes = get_df_bytes(df)\n",
    "\n",
    "#     # Upload parquet bytes to adls\n",
    "#     adls_client.upload(path, parquet_bytes)  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "save_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
